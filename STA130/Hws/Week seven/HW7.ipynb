{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174bcac4",
   "metadata": {},
   "source": [
    "# STA130 Homework 07\n",
    "\n",
    "This is my submission for Homework 07, answering each question based on my understanding and work with ChatGPT to enhance clarity and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c873bcb",
   "metadata": {},
   "source": [
    "### Question 1: Differences Between Types of Regression and Model Behavior with Variables\n",
    "\n",
    "**1. Simple Linear Regression vs. Multiple Linear Regression**\n",
    "\n",
    "In Simple Linear Regression, we predict the outcome variable using one predictor variable. The equation is of the form:\n",
    "\n",
    "$$ outcome = \\beta_0 + \\beta_1 \\cdot predictor + \\epsilon $$\n",
    "\n",
    "In contrast, Multiple Linear Regression includes two or more predictors, which enhances the model's capacity to capture more complex relationships. The equation for Multiple Linear Regression is:\n",
    "\n",
    "$$ outcome = \\beta_0 + \\beta_1 \\cdot predictor_1 + \\beta_2 \\cdot predictor_2 + ... + \\epsilon $$\n",
    "\n",
    "**2. Continuous vs. Indicator Variables in Simple Linear Regression**\n",
    "\n",
    "A continuous variable can take any value within a range, whereas an indicator variable represents categories as binary values (0 or 1). For example, we might have an indicator variable 1(\\text{Male}) to represent gender.\n",
    "\n",
    "In simple linear regression, the model with a continuous predictor is:\n",
    "\n",
    "$$ outcome = \\beta_0 + \\beta_1 \\cdot continuous\\ predictor $$\n",
    "\n",
    "For an indicator variable:\n",
    "\n",
    "$$ outcome = \\beta_0 + \\beta_1 \\cdot 1(indicator) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ab609",
   "metadata": {},
   "source": [
    "**3. Changes in Model Behavior with Indicator and Continuous Variables**\n",
    "\n",
    "When a continuous and indicator variable are both included in a Multiple Linear Regression model, the behavior of the outcome adjusts depending on the category indicated by the indicator variable.\n",
    "\n",
    "$$ outcome = \\beta_0 + \\beta_1 \\cdot continuous + \\beta_2 \\cdot 1(indicator) $$\n",
    "\n",
    "This allows the model to capture different trends within each category indicated by the variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1f65b2",
   "metadata": {},
   "source": [
    "**4. Interaction Effects between Continuous and Indicator Variables**\n",
    "\n",
    "Adding an interaction term between a continuous and indicator variable enables the model to capture how the effect of the continuous variable changes based on the category. The equation becomes:\n",
    "\n",
    "$$ outcome = \\beta_0 + \\beta_1 \\cdot continuous + \\beta_2 \\cdot 1(indicator) + \\beta_3 \\cdot continuous \\cdot 1(indicator) $$\n",
    "\n",
    "This provides flexibility, especially useful when the effect of one variable depends on another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f67cb7a",
   "metadata": {},
   "source": [
    "**5. Multiple Linear Regression with Only Indicator Variables**\n",
    "\n",
    "When using only indicator variables from a non-binary categorical variable, we need $k-1$ indicator variables to encode a categorical variable with $k$ categories. This approach introduces a baseline group and offsets for each other group.\n",
    "\n",
    "$$ outcome = \\beta_0 + \\beta_1 \\cdot 1(cat_1) + \\beta_2 \\cdot 1(cat_2) + ... $$\n",
    "\n",
    "Each indicator variable adjusts the intercept based on the category, allowing flexible prediction based on category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee106b95",
   "metadata": {},
   "source": [
    "### Question 2: Predicting Advertising Effectiveness with Interaction Effects\n",
    "\n",
    "**Identifying Variables**\n",
    "\n",
    "For this scenario, the outcome variable might be `Sales` or `Effectiveness`. The predictors are `TV_ad_budget` and `online_ad_budget`.\n",
    "\n",
    "We might consider a basic linear form as:\n",
    "\n",
    "$$ Sales = \\beta_0 + \\beta_1 \\cdot TV\\_ad\\_budget + \\beta_2 \\cdot online\\_ad\\_budget $$\n",
    "\n",
    "With interaction:\n",
    "\n",
    "$$ Sales = \\beta_0 + \\beta_1 \\cdot TV\\_ad\\_budget + \\beta_2 \\cdot online\\_ad\\_budget + \\beta_3 \\cdot (TV\\_ad\\_budget \\times online\\_ad\\_budget) $$\n",
    "\n",
    "This interaction term lets us examine if the effectiveness of one ad channel depends on the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4164a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with high and low budgets (using binary encoding)\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Sample data setup\n",
    "ads_data = pd.DataFrame({\n",
    "    'TV_ad_budget': ['high', 'low', 'high', 'low'],\n",
    "    'online_ad_budget': ['high', 'high', 'low', 'low'],\n",
    "    'sales': [300, 200, 150, 100]\n",
    "})\n",
    "\n",
    "# Encoding\n",
    "ads_data['TV_high'] = (ads_data['TV_ad_budget'] == 'high').astype(int)\n",
    "ads_data['Online_high'] = (ads_data['online_ad_budget'] == 'high').astype(int)\n",
    "\n",
    "# Fit model with interaction\n",
    "model = smf.ols('sales ~ TV_high * Online_high', data=ads_data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eacb05",
   "metadata": {},
   "source": [
    "### Question 3: Logistic Regression and Model Building with CSCS Data\n",
    "\n",
    "**Logistic Regression Setup**\n",
    "\n",
    "In this question, I explored using logistic regression instead of multiple linear regression because we are predicting a binary outcome. Using categorical and continuous variables, we created a logistic model.\n",
    "\n",
    "The following code demonstrates the setup of a logistic regression model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression example setup\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Sample data for illustration\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df['FireType'] = (df['Type 1'] == 'Fire').astype(int)\n",
    "\n",
    "# Logistic regression example\n",
    "model_spec = 'FireType ~ Attack * Legendary + Defense * (Type 2 == \"None\") + C(Generation)'\n",
    "log_reg = smf.logit(model_spec, data=df).fit()\n",
    "log_reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d970c",
   "metadata": {},
   "source": [
    "**Interpretation of Logistic Model Output**\n",
    "\n",
    "Logistic regression results are presented in terms of log-odds, where positive coefficients indicate an increase in odds as the predictor increases. In this case, the coefficients explain how being a 'Fire' type relates to attack, defense, and whether it is a Legendary Pokemon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615fe79",
   "metadata": {},
   "source": [
    "### Question 4: Contradictions between Model Fit and Coefficients\n",
    "\n",
    "It is possible for a model to have high-value coefficients and significant p-values while explaining only a small amount of variability (low R-squared). This discrepancy arises because R-squared measures the overall variance explained by the model, while p-values indicate the strength of evidence against the null hypothesis for individual coefficients.\n",
    "\n",
    "$$ R^2 = 1 - \\frac{\\sum_{i=1}^n(y_i - \\hat{y})^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2} $$\n",
    "\n",
    "Thus, significant p-values on coefficients do not guarantee a high R-squared and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0258de52",
   "metadata": {},
   "source": [
    "### Question 5: Model Performance with Train-Test Split\n",
    "\n",
    "Here we use a 50-50 split to evaluate in-sample and out-of-sample R-squared to check generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and split data\n",
    "np.random.seed(130)\n",
    "df_train, df_test = train_test_split(df, test_size=0.5)\n",
    "\n",
    "# Model fit and R-squared calculation\n",
    "simple_model = smf.ols('HP ~ Attack + Defense', data=df_train).fit()\n",
    "in_sample_r2 = simple_model.rsquared\n",
    "out_sample_r2 = np.corrcoef(df_test['HP'], simple_model.predict(df_test))[0,1]**2\n",
    "\n",
    "print(f\"In-sample R-squared: {in_sample_r2}\")\n",
    "print(f\"Out-of-sample R-squared: {out_sample_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f359dd9",
   "metadata": {},
   "source": [
    "### Question 6: Multicollinearity and Condition Number in Design Matrix\n",
    "\n",
    "High multicollinearity, indicated by a high condition number, shows that predictors are interdependent. This complicates coefficient interpretation and reduces model reliability. Condition numbers above 30 may indicate potential issues.\n",
    "\n",
    "Centering and scaling the data helps correct inflated condition numbers as demonstrated below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import center, scale\n",
    "\n",
    "# Centering and Scaling for better Condition Number\n",
    "scaled_model_spec = smf.ols('HP ~ scale(center(Attack)) + scale(center(Defense))', data=df_train).fit()\n",
    "scaled_model_spec.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa80752",
   "metadata": {},
   "source": [
    "### Question 7: Model Development and Complexity Trade-offs\n",
    "\n",
    "As we develop models by extending previous versions, we balance complexity and interpretability. Simpler models with reasonable fit, like `model6`, may offer better generalizability over more complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332a89a",
   "metadata": {},
   "source": [
    "### Question 8: Variation in Model Performance Metrics\n",
    "\n",
    "This question involves observing the variation in in-sample and out-of-sample R-squared over multiple train-test splits to examine model stability and risk of overfitting.\n",
    "\n",
    "```python\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "reps = 100\n",
    "in_sample = []\n",
    "out_sample = []\n",
    "\n",
    "for _ in range(reps):\n",
    "    train, test = train_test_split(df, test_size=0.5)\n",
    "    model = smf.ols('HP ~ Attack + Defense', data=train).fit()\n",
    "    in_sample.append(model.rsquared)\n",
    "    out_sample.append(np.corrcoef(test['HP'], model.predict(test))[0,1]**2)\n",
    "\n",
    "# Visualization\n",
    "px.scatter(x=in_sample, y=out_sample, labels={'x':'In-Sample R^2', 'y':'Out-of-Sample R^2'})\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb9ed0",
   "metadata": {},
   "source": [
    "### Question 9: Interpreting Future Prediction Reliability\n",
    "\n",
    "Using a model to predict outcomes for future data (e.g., new generations) emphasizes generalizability. If a model fit on previous data poorly predicts new data, it indicates overfitting, a lack of generalizability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0a5bb",
   "metadata": {},
   "source": [
    "### Chat Summary and Link\n",
    "\n",
    "For this homework, I consulted with ChatGPT to help clarify complex questions and ensure accurate explanations. In particular, the chatbot provided detailed insights into differences between linear and logistic regression, the importance of model interpretability, and techniques to evaluate model performance and generalizability.\n",
    "\n",
    "Key takeaways from this chat include:\n",
    "\n",
    "- **Simple vs. Multiple Linear Regression:** Clear understanding of model expansion and added predictive power with additional predictors.\n",
    "- **Logistic Regression and Interaction Effects:** The benefits of logistic regression for binary outcomes and modeling interactions.\n",
    "- **Overfitting and Generalizability:** Recognizing overfitting through out-of-sample testing and the role of condition numbers in multicollinearity detection.\n",
    "- **Model Building Approach:** Balancing complexity with interpretability for reliable predictions and insights.\n",
    "\n",
    "Link to the chat: [Chat Summary and Link](https://chatgpt.com/share/67354dc3-23ec-8011-aa6e-e6b6827e68ca)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
